{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3elt32UBha9"
   },
   "source": [
    "## HW1 LDA\n",
    "The objective of this assignment is to introduce you to the Latent Dirichlet Allocation (LDA) model, one of the fundamental techniques in the field of topic modeling, and apply it to analyze real-world Twitter data related to COVID-19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YUDhlbNwCust",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (3.4.1)\n",
      "Requirement already satisfied: funcy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (2.0)\n",
      "Requirement already satisfied: tzdata in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (2023.3)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (2.1.1)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis funcy tzdata --no-dependencies\n",
    "!pip install pandas nltk gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8X-j6QWFBmDx"
   },
   "source": [
    "### Data Preprocessing\n",
    "The first step is loading dataset and conducting data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aNhOss4JBp6O"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcovid_tweet_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m tweets \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv('covid_tweet_dataset.csv')\n",
    "tweets = df[\"tweet\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoI3aRvGB0cZ"
   },
   "source": [
    "#### Remove HTTP links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jkzSODReTkS_"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Search URLs in a given tweet string using regex\n",
    "# \"search\" method and extracting the starting index\n",
    "# of the instance if found. \n",
    "# Then, as all URLs were found to be 23 characters\n",
    "# in length, replace the 23-character-long string\n",
    "# starting at \"h\" with an empty string using regex\n",
    "# \"sub\" method.\n",
    "def remove_urls(tweets):\n",
    "    index = 0\n",
    "    while index < len(tweets):\n",
    "        tweet = tweets[index]\n",
    "        x = re.search(\"https://\", tweet)\n",
    "        if x:\n",
    "            url = tweet[x.start():(x.start() + 23)]\n",
    "            tweets[index] = re.sub(url, \"\", tweet)\n",
    "            tweet = tweets[index]\n",
    "        y = re.search(\"https://\", tweet)\n",
    "        if y:\n",
    "            url = tweet[y.start():(y.start() + 23)]\n",
    "            tweets[index] = re.sub(url, \"\", tweet)\n",
    "        index += 1\n",
    "    return tweets\n",
    "\n",
    "tweets = remove_urls(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-w9D7QWPCoKO"
   },
   "source": [
    "#### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFY1JMVKRV2m"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "# Using an online list of NLTK stop words\n",
    "# as a reference point, each word is searched\n",
    "# and substituted using the regex \"sub\" feature.\n",
    "#\n",
    "# (This is due to the fact that I was unable to\n",
    "# resolve an error with NLTK's stopwords.\n",
    "# I spent the better part of two hours hoping\n",
    "# to find a solution, but was unable to.\n",
    "# This is my workaround.)\n",
    "def remove_stopwords(tweets):\n",
    "    index = 0\n",
    "    while index < len(tweets):\n",
    "        tweet = tweets[index].lower()\n",
    "        spaced = re.sub('\\s', \" \", tweet)\n",
    "        # Word searches include spaces directly\n",
    "        # before and after the word in question\n",
    "        # in order to avoid instances where, for\n",
    "        # example, the word 'in' is being\n",
    "        # searched, and is found and deleted due\n",
    "        # to its place in a larger word such as\n",
    "        # 'Minnesota' or 'shopping.'\n",
    "        a = re.sub(\" for \", \" \", spaced)\n",
    "        aa = re.sub(\" those \", \" \", a)\n",
    "        aaa = re.sub(\" your \", \" \", aa)\n",
    "        aaaa = re.sub(\" can \", \" \", aaa)\n",
    "        b = re.sub(\"because\", \"\", aaaa)\n",
    "        bb = re.sub(\" were \", \" \", b)\n",
    "        bbb = re.sub(\" their \", \" \", bb)\n",
    "        bbbb = re.sub(\" also \", \" \", bbb)\n",
    "        c = re.sub(\" if \", \" \", bbbb)\n",
    "        cc = re.sub(\" has \", \" \", c)\n",
    "        ccc = re.sub(\" they \", \" \", cc)\n",
    "        cccc = re.sub(\" other \", \" \", ccc)\n",
    "        d = re.sub(\" are \", \" \", cccc)\n",
    "        dd = re.sub(\" been \", \" \", d)\n",
    "        ddd = re.sub(\" our \", \" \", dd)\n",
    "        dddd = re.sub(\" without \", \" \", ddd)\n",
    "        e = re.sub(\" that \", \" \", dddd)\n",
    "        ee = re.sub(\" having \", \" \", e)\n",
    "        eee = re.sub(\" here \", \" \", ee)\n",
    "        eeee = re.sub(\" only \", \" \", eee)\n",
    "        f = re.sub(\" this \", \" \", eeee)\n",
    "        ff = re.sub(\" be \", \" \", f)\n",
    "        fff = re.sub(\" i'm \", \" \", ff)\n",
    "        ffff = re.sub(\" so \", \" \", fff)\n",
    "        g = re.sub(\" we \", \" \", ffff)\n",
    "        gg = re.sub(\" do \", \" \", g)\n",
    "        ggg = re.sub(\" i'd \", \" \", gg)\n",
    "        gggg = re.sub(\" like \", \" \", ggg)\n",
    "        h = re.sub(\" my \", \" \", gggg)\n",
    "        hh = re.sub(\" was \", \" \", h)\n",
    "        hhh = re.sub(\" is \", \" \", hh)\n",
    "        hhhh = re.sub(\" such \", \" \", hhh)\n",
    "        i = re.sub(\" up \", \" \", hhhh)\n",
    "        ii = re.sub(\" had \", \" \", i)\n",
    "        iii = re.sub(\" you're \", \" \", ii)\n",
    "        iiii = re.sub(\" no \", \" \", iii)\n",
    "        j = re.sub(\" you \", \" \", iiii)\n",
    "        jj = re.sub(\" being \", \" \", j)\n",
    "        jjj = re.sub(\" they're \", \" \", jj)\n",
    "        jjjj = re.sub(\"would\", \"\", jjj)\n",
    "        k = re.sub(\" me \", \" \", jjjj)\n",
    "        kk = re.sub(\" am \", \" \", k)\n",
    "        l = re.sub(\" i \", \" \", kk)\n",
    "        ll = re.sub(\" did \", \" \", l)\n",
    "        lll = re.sub(\" didn't \", \" \", ll)\n",
    "        llll = re.sub(\" isn't \", \" \", lll)\n",
    "        m = re.sub(\" in \", \" \", llll)\n",
    "        mm = re.sub(\" but \", \" \", m)\n",
    "        mmm = re.sub(\" will \", \" \", mm)\n",
    "        mmmm = re.sub(\"could\", \"\", mmm)\n",
    "        n = re.sub(\" to \", \" \", mmmm)\n",
    "        nn = re.sub(\" while \", \" \", n)\n",
    "        nnn = re.sub(\" im \", \" \", nn)\n",
    "        nnnn = re.sub(\" any \", \" \", nnn)\n",
    "        o = re.sub(\" as \", \" \", nnnn)\n",
    "        oo = re.sub(\" with \", \" \", o)\n",
    "        ooo = re.sub(\" after \", \" \", oo)\n",
    "        oooo = re.sub(\" one \", \" \", ooo)\n",
    "        p = re.sub(\" and \", \" \", oooo)\n",
    "        pp = re.sub(\" against \", \" \", p)\n",
    "        ppp = re.sub(\" it's \", \" \", pp)\n",
    "        pppp = re.sub(\" who \", \" \", ppp)\n",
    "        q = re.sub(\" at \", \" \", pppp)\n",
    "        qq = re.sub(\" between \", \" \", q)\n",
    "        qqq = re.sub(\" it \", \" \", qq)\n",
    "        qqqq = re.sub(\" his \", \" \", qqq)\n",
    "        r = re.sub(\" how \", \" \", qqqq)\n",
    "        rr = re.sub(\" during \", \" \", r)\n",
    "        rrr = re.sub(\" its \", \" \", rr)\n",
    "        rrrr = re.sub(\" her \", \" \", rrr)\n",
    "        s = re.sub(\" the \", \" \", rrrr)\n",
    "        ss = re.sub(\" before \", \" \", s)\n",
    "        sss = re.sub(\" can \", \" \", ss)\n",
    "        ssss = re.sub(\" she \", \" \", sss)\n",
    "        t = re.sub(\" a \", \" \", ssss)\n",
    "        tt = re.sub(\" through \", \" \", t)\n",
    "        ttt = re.sub(\" don't \", \" \", tt)\n",
    "        tttt = re.sub(\" he \", \" \", ttt)\n",
    "        u = re.sub(\" an \", \" \", tttt)\n",
    "        uu = re.sub(\" to \", \" \", u)\n",
    "        uuu = re.sub(\" come \", \" \", uu)\n",
    "        uuuu = re.sub(\" just \", \" \", uuu)\n",
    "        v = re.sub(\" of \", \" \", uuuu)\n",
    "        vv = re.sub(\" about \", \" \", v)\n",
    "        vvv = re.sub(\" going \", \" \", vv)\n",
    "        vvvv = re.sub(\" go \", \" \", vvv)\n",
    "        w = re.sub(\" on \", \" \", vvvv)\n",
    "        ww = re.sub(\" from \", \" \", w)\n",
    "        www = re.sub(\" us \", \" \", ww)\n",
    "        wwww = re.sub(\" not \", \" \", www)\n",
    "        x = re.sub(\" or \", \" \", wwww)\n",
    "        xx = re.sub(\" down \", \" \", x)\n",
    "        xxx = re.sub(\" doing \", \" \", xx)\n",
    "        xxxx = re.sub(\" what \", \" \", xxx)\n",
    "        y = re.sub(\" have \", \" \", xxxx)\n",
    "        yy = re.sub(\" off \", \" \", y)\n",
    "        yyy = re.sub(\" then \", \" \", yy)\n",
    "        yyyy = re.sub(\" who \", \" \", yyy)\n",
    "        z = re.sub(\" by \", \" \", yyyy)\n",
    "        zz = re.sub(\" over \", \" \", z)\n",
    "        zzz = re.sub(\" into \", \" \", zz)\n",
    "        zzzz = re.sub(\" where \", \" \", zzz)\n",
    "        tweets[index] = zzzz\n",
    "        index += 1\n",
    "    return tweets\n",
    "\n",
    "tweets = remove_stopwords(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvSDI_ktDqQC"
   },
   "source": [
    "#### Remove neumerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lMf-ioFRnk-"
   },
   "outputs": [],
   "source": [
    "# Utilizes regex's built-in features for\n",
    "# searching and substituting numbers from\n",
    "# strings of text.\n",
    "def remove_numbers(tweets):\n",
    "    index = 0\n",
    "    while index < len(tweets):\n",
    "        x = re.sub(r'[0-9]', '', tweets[index])\n",
    "        tweets[index] = x\n",
    "        index += 1\n",
    "    return tweets\n",
    "\n",
    "tweets = remove_numbers(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvSDI_ktDqQC"
   },
   "source": [
    "#### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lMf-ioFRnk-"
   },
   "outputs": [],
   "source": [
    "# Utilizing string methods in Python, we may\n",
    "# remove any instances of punctuation from each\n",
    "# tweet.\n",
    "def remove_punctuation(tweets):\n",
    "    index = 0\n",
    "    while index < len(tweets):\n",
    "        tweet = tweets[index]\n",
    "        tweets[index] = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "        index += 1\n",
    "    return tweets\n",
    "\n",
    "tweets = remove_punctuation(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Cp41FnQDoLc"
   },
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1k4LysggRiEn"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize tweets one by one before appending\n",
    "# to a previously blank array.\n",
    "def tokenize(tweets):\n",
    "    tokens = []\n",
    "    for tweet in tweets:\n",
    "        tokens.append(word_tokenize(tweet))\n",
    "    return tokens\n",
    "\n",
    "tweets = tokenize(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2egIyJg_DsPX"
   },
   "source": [
    "###LDA Implementation\n",
    "Once you have preprocessed the Twitter data, the next step is to implement the Latent Dirichlet Allocation (LDA) model using the Gensim library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pINChP0ID0HP"
   },
   "source": [
    "#### Bag of Words\n",
    "We first build a dictionary including all words from `tweets` and then convert `tweets` to bag-of-word format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubIWgrReV6mn"
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# create dictionary\n",
    "text_dict = Dictionary(tweets)\n",
    "# generate bag-of-word format data\n",
    "tweets_bow = [text_dict.doc2bow(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vx439-OsD7Xm"
   },
   "source": [
    "#### LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucobS9LXr1P"
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# Assign tweets_bow to corpus.\n",
    "corpus = tweets_bow\n",
    "\n",
    "# Initialize LDA Model using pyLDAvis\n",
    "# gensim.\n",
    "tweets_lda = LdaModel(corpus, num_topics=10, id2word=text_dict)\n",
    "\n",
    "tweets_lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0FzuEzibmsN"
   },
   "source": [
    "#### Visualization\n",
    "After fitting a LDA model, you can visualize the topic distribution generated by the LDA model to understand the latent topics within tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QoGTkb2qblZA"
   },
   "outputs": [],
   "source": [
    "# visualization tool\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(tweets_lda, tweets_bow, tweets_lda.id2word)\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPNXfUsQScbmDdP0M2Po6LN",
   "provenance": [
    {
     "file_id": "1TttkxSEVZwM4qY92dlBw8wyHO4w1bH0o",
     "timestamp": 1694037425649
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
