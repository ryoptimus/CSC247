{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scyE4k8gpwl0"
      },
      "source": [
        "# HW2 BiLSTM for PoS tagging\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this project, you will use PyTorch to implement a BiLSTM for the PoS tagging task. We use the Universal Dependencies English Web Treebank (UDPOS) dataset, which is provided by the TorchText library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9osUMmompwl2",
        "outputId": "344f7834-7450-4991-81d7-c4cee6bd0f96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.6.0 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (3.0.4)\n",
            "0.6.0\n"
          ]
        }
      ],
      "source": [
        "! pip install torchtext==0.6.0\n",
        "! pip install Cython\n",
        "#! pip install -U pip setuptools wheel\n",
        "#! pip install -U spacy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "print(torchtext.__version__)\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "import random\n",
        "SEED = 6\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTuoKelVpwl4"
      },
      "source": [
        "## Data Preprocessing\n",
        "The key parts of TorchText is the `Field`. The `Field` handles how your dataset is processed. You can find more detiails in official document (https://torchtext.readthedocs.io/en/latest/data.html#field).\n",
        "\n",
        "Here, the `TEXT` field handles how the  text is dealt with. You can use  `lower = True` to make texts written in lowercases.\n",
        "\n",
        "Next, define the `Fields` for the tags. This dataset actually has two different sets of tags, [universal dependency (UD) tags](https://universaldependencies.org/u/pos/) and [Penn Treebank (PTB) tags](https://www.sketchengine.eu/penn-treebank-tagset/). In this project, you can only train the model on the UD tags. `UD_TAGS` handles how the UD tags should be handled. Our `TEXT` vocabulary - which we'll build later - will have *unknown* tokens in it, i.e. tokens that are not within our vocabulary. However, we won't have unknown tags as we are dealing with a finite set of possible tags. TorchText `Fields` initialize a default unknown token, `<unk>`, which we remove by setting `unk_token = None`. `PTB_TAGS` does the same as `UD_TAGS`, but handles the PTB tags instead.\n",
        "\n",
        "Then, define `fields`, which handles passing our fields to the dataset.\n",
        "\n",
        "Next, load the UDPOS dataset using the defined fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fidoBBJfpwl5",
        "outputId": "ffad2336-8f67-461b-8971-df774c8b1458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 12543\n",
            "Number of validation examples: 2002\n",
            "Number of testing examples: 2077\n"
          ]
        }
      ],
      "source": [
        "TXT = data.Field(lower = True)\n",
        "UD_TAGS = data.Field(unk_token = None)\n",
        "PTB_TAGS = data.Field(unk_token = None)\n",
        "fields = ((\"text\", TXT), (\"udtags\", UD_TAGS), (\"ptbtags\", PTB_TAGS))\n",
        "train_data, valid_data, test_data = datasets.UDPOS.splits(fields)\n",
        "\n",
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cHlSTlEpwl7"
      },
      "source": [
        "### Statistics of Data\n",
        "\n",
        "After loading the data, you will do some data anslysis.\n",
        "\n",
        "(1) get the most common first $10$  tokens in texts.  \n",
        "(2) get all the possible UD tags.  \n",
        "(3) compute the number of samples for each UD tag.  \n",
        "\n",
        "tips:\n",
        "(1) The __vars__ function can be used to split the sample into differnet atoms.  You can also print some of them directly.   \n",
        "(2) For better statistics, we can transform the texts into numeric representations. The `build_vocab` is used for this purpose (see anie.me/On-Torchtext/). For texts, we want some unknown tokens within our dataset in order to replicate how this model would be used in real life, so we set the `min_freq` to 2 which means only tokens that appear twice in the training set will be added to the vocabulary and the rest will be replaced by `<unk>` tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCznT-c3pwl8",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e79680d-4f92-4dd3-bb3b-cfee7915fd47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:39, 5.40MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:22<00:00, 18120.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NOUN', 34781), ('PUNCT', 23679), ('VERB', 23081), ('PRON', 18577), ('ADP', 17638), ('DET', 16285), ('PROPN', 12946), ('ADJ', 12477), ('AUX', 12343), ('ADV', 10548)]\n",
            "['<pad>', 'NOUN', 'PUNCT', 'VERB', 'PRON', 'ADP', 'DET', 'PROPN', 'ADJ', 'AUX', 'ADV', 'CCONJ', 'PART', 'NUM', 'SCONJ', 'X', 'INTJ', 'SYM']\n"
          ]
        }
      ],
      "source": [
        "def data_analysis(TXT, UD_TAGS, train_data, valid_data, test_data):\n",
        "    TXT.build_vocab(train_data, min_freq=2, vectors=\"glove.6B.100d\")\n",
        "    UD_TAGS.build_vocab(train_data)\n",
        "    PTB_TAGS.build_vocab(train_data)\n",
        "    print(UD_TAGS.vocab.freqs.most_common(10))\n",
        "    print(UD_TAGS.vocab.itos)\n",
        "\n",
        "data_analysis(TXT, UD_TAGS, train_data, valid_data, test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOu9Uv6ZpwmL"
      },
      "source": [
        "In the final stage of data preparation, we focus on configuring the iterator. This iterator will be used to provide batches of data for processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78DjTNiJpwmL"
      },
      "outputs": [],
      "source": [
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-KCF_thpwmO"
      },
      "source": [
        "## Building the Model\n",
        "\n",
        "\n",
        "Next, we build our model - a multi-layer bi-directional LSTM.  The model takes in a sequence of tokens, $X = \\{x_1, x_2,...,x_T\\}$, passes them through an embedding layer, $e$, to get the token embeddings, $e(X) = \\{e(x_1), e(x_2), ..., e(x_T)\\}$.\n",
        "\n",
        "These embeddings are processed - one per time-step - by the forward and backward LSTMs. The forward LSTM processes the sequence from left-to-right, whilst the backward LSTM processes the sequence right-to-left, i.e. the first input to the forward LSTM is $x_1$ and the first input to the backward LSTM is $x_T$. The LSTMs also take in the the hidden, $h$, and cell, $c$, states from the previous time-step After the whole sequence has been processed, the hidden and cell states are then passed to the next layer of the LSTM. The initial hidden and cell states, $h_0$ and $c_0$, for each direction and layer are initialized to a tensor full of zeros. We then concatenate both the forward and backward hidden states from the final layer of the LSTM, $H = \\{h_1, h_2, ... h_T\\}$, where $h_1 = [h^{\\rightarrow}_1;h^{\\leftarrow}_T]$, $h_2 = [h^{\\rightarrow}_2;h^{\\leftarrow}_{T-1}]$, etc. and pass them through a linear layer, $f$, which is used to make the prediction of which tag applies to this token, $\\hat{y}_t = f(h_t)$.\n",
        "\n",
        "We implement the model detailed above in the `BiLSTM` class. You can use the provided `embedding`, `lstm`, and `linear` module in PyTorch library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-Wa3Tp5pwmO"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 embedding_dim,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 pad_idx):\n",
        "\n",
        "        super(BiLSTM, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, text):\n",
        "        embeddings = self.embedding(text)\n",
        "\n",
        "        #embeddings = [sent len, batch size, emb dim]\n",
        "\n",
        "        #pass embeddings into LSTM\n",
        "        outputs, (hidden, cell) = self.lstm(embeddings)\n",
        "\n",
        "        #outputs holds the backward and forward hidden states in the final layer\n",
        "        #hidden and cell are the backward and forward hidden and cell states at the final time-step\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #use our outputs to make a prediction of what the tag should be\n",
        "        predictions = self.fc(outputs)\n",
        "\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTjQq-Eqpwmi"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "Moving forward, we proceed with the instantiation of our model. To ensure compatibility, we must confirm that the embedding dimensions match those of the GloVe embeddings we previously loaded.\n",
        "\n",
        "The remaining hyperparameters have been carefully selected as reasonable defaults. However, it's worth noting that there might exist alternative combinations that could yield improved performance on this specific model and dataset.\n",
        "\n",
        "For the input and output dimensions, we directly use the lengths of the respective vocabularies. To obtain the padding index, we extract it from the vocabulary and the Field associated with the text data. This ensures that our model is properly configured to handle the input and output dimensions while accommodating padding appropriately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crDf8hTHpwmi"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(TXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = len(UD_TAGS.vocab)\n",
        "N_LAYERS = 1\n",
        "BIDIRECTIONAL = True\n",
        "PAD_IDX = TXT.vocab.stoi[TXT.pad_token]\n",
        "\n",
        "model = BiLSTM(INPUT_DIM,\n",
        "                EMBEDDING_DIM,\n",
        "                HIDDEN_DIM,\n",
        "                OUTPUT_DIM,\n",
        "                N_LAYERS,\n",
        "                BIDIRECTIONAL,\n",
        "                PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc0xJFAApwmi"
      },
      "source": [
        "We'll now initialize our model's embedding layer with the pre-trained embedding values we loaded earlier.\n",
        "\n",
        "This is done by getting them from the vocab's `.vectors` attribute and then performing a `.copy` to overwrite the embedding layer's current weights.\n",
        "\n",
        "Notice: You have loaded the `Glove` embedding in the data processing step, stored in `TEXT.vocab.vectors`. It means that the embedding layer here is used to transform the texts into the numeric representations for the input of the following LSTM cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEq91x0Epwmj"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "def apply_embeddings(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
        "    return model\n",
        "model = apply_embeddings(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8EQh0BGpwmn"
      },
      "source": [
        "Now, let's move on to defining the  loss function and optimizer for our model.\n",
        "\n",
        "While it's important to note that our tag vocabulary doesn't contain <unk> tokens, we do have <pad> tokens. These <pad> tokens are introduced to ensure that all sentences within a batch have the same length, a requirement for efficient processing. However, we don't want our model to learn to predict or generate <pad> tokens, as they don't carry meaningful information.\n",
        "\n",
        "To address this, there is a crucial adjustment when defining the loss function. Specifically, you should set the __`ignore_index`__ parameter in the loss function to the index corresponding to the <pad> token in the tag vocabulary. By doing this, you can  effectively instruct the loss function to disregard <pad> tokens when computing the loss during training. This ensures that the model focuses on learning the meaningful tags while ignoring the <pad> tokens, which are essentially placeholders for padding purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOhdfyscpwmo"
      },
      "outputs": [],
      "source": [
        "TAG_PAD_IDX = UD_TAGS.vocab.stoi[UD_TAGS.pad_token]\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIgmzjvEpwmq"
      },
      "source": [
        "You will implement the function to compute the accuracy to evaluate the model performance.\n",
        "\n",
        "The issue is that we don't want to calculate accuracy over the `<pad>` tokens as we aren't interested in predicting them.\n",
        "\n",
        "The function below only calculates accuracy over non-padded tokens. `non_pad_elements` is a tensor containing the indices of the non-pad tokens within an input batch. We then compare the predictions of those elements with the labels to get a count of how many predictions were correct. We then divide this by the number of non-pad elements to get our accuracy value over the batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6FMQ9Pppwmq"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "def categorical_accuracy(preds, y, tag_pad_idx):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch as a decimal\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # find max probability index\n",
        "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
        "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
        "\n",
        "    return correct.sum() / y[non_pad_elements].shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzbRWYkIpwmq"
      },
      "source": [
        "Next is the function that handles training our model.\n",
        "\n",
        "To initiate the training process, our initial step involves setting the model to '`train`' mode. This action activates certain techniques such as dropout and batch normalization if they are being employed. Subsequently, we proceed by iterating over our data iterator, which provides us with a batch of training examples.\n",
        "\n",
        "Within each iteration of the training loop, the following actions are carried out:\n",
        "\n",
        "1. Resetting the gradients for all model parameters, essentially zeroing them out, to prepare for the upcoming gradient calculation.\n",
        "2. Feeding the batch of text data into our model to generate predictions.\n",
        "3. Since PyTorch loss functions expect predictions to be in a particular shape, we reshape our model's predictions accordingly.\n",
        "4. Compute both the loss and accuracy by comparing the predicted tags with the actual tags.\n",
        "5. Utilize the 'backward' method to compute the gradients of the model's parameters with respect to the loss.\n",
        "6. Execute an 'optimizer step' to update the model's parameters based on the calculated gradients.\n",
        "7. Maintain running totals for both loss and accuracy to monitor the model's performance.  \n",
        "\n",
        "\n",
        "This process is integral to the training of our model, and each step plays a crucial role in improving its ability to make accurate predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QGjrEYppwmr"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in iterator:\n",
        "        text = batch.text\n",
        "        tags = batch.udtags\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        predictions = model(text)\n",
        "\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        #tags = [sent len, batch size]\n",
        "\n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        tags = tags.view(-1)\n",
        "\n",
        "        #predictions = [sent len * batch size, output dim]\n",
        "        #tags = [sent len * batch size]\n",
        "\n",
        "        loss = criterion(predictions, tags)\n",
        "        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkdJ4J-Gpwms"
      },
      "source": [
        "The evaluate function is similar to train but with key differences. We set the model to evaluation mode using model.eval(), which disables dropout and batch normalization. We use `torch.no_grad()` to avoid gradient calculation and skip `optimizer.zero_grad()` and `optimizer.step()` as we don't update model parameters during evaluation. This function is designed to assess model performance without altering its parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoxU16Sbpwms"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in iterator:\n",
        "            text = batch.text\n",
        "            tags = batch.udtags\n",
        "\n",
        "            predictions = model(text)\n",
        "\n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "\n",
        "            loss = criterion(predictions, tags)\n",
        "            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlWPogO8pwmv"
      },
      "source": [
        "Now, let's move on to the final phase of training our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W08Crfvzpwmw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee276dfa-3c85-4484-f500-bf303a47f847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 \n",
            "\tTrain Loss: 1.392 | Train Acc: 60.84%\n",
            "\t Val. Loss: 0.729 |  Val. Acc: 82.18%\n",
            "Epoch: 02 \n",
            "\tTrain Loss: 0.376 | Train Acc: 88.94%\n",
            "\t Val. Loss: 0.512 |  Val. Acc: 85.75%\n",
            "Epoch: 03 \n",
            "\tTrain Loss: 0.258 | Train Acc: 91.82%\n",
            "\t Val. Loss: 0.465 |  Val. Acc: 86.28%\n",
            "Epoch: 04 \n",
            "\tTrain Loss: 0.214 | Train Acc: 92.90%\n",
            "\t Val. Loss: 0.449 |  Val. Acc: 86.58%\n",
            "Epoch: 05 \n",
            "\tTrain Loss: 0.188 | Train Acc: 93.69%\n",
            "\t Val. Loss: 0.442 |  Val. Acc: 86.82%\n",
            "Epoch: 06 \n",
            "\tTrain Loss: 0.167 | Train Acc: 94.38%\n",
            "\t Val. Loss: 0.448 |  Val. Acc: 86.59%\n",
            "Epoch: 07 \n",
            "\tTrain Loss: 0.148 | Train Acc: 95.09%\n",
            "\t Val. Loss: 0.459 |  Val. Acc: 86.14%\n",
            "Epoch: 08 \n",
            "\tTrain Loss: 0.133 | Train Acc: 95.61%\n",
            "\t Val. Loss: 0.460 |  Val. Acc: 86.15%\n",
            "Epoch: 09 \n",
            "\tTrain Loss: 0.118 | Train Acc: 96.13%\n",
            "\t Val. Loss: 0.478 |  Val. Acc: 86.04%\n",
            "Epoch: 10 \n",
            "\tTrain Loss: 0.105 | Train Acc: 96.59%\n",
            "\t Val. Loss: 0.487 |  Val. Acc: 86.39%\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "N_EPOCHS = 10\n",
        "\n",
        "model_save_path = 'hw2_bilstm.pt'\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, TAG_PAD_IDX)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} ')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMKuvg9apwmw"
      },
      "source": [
        "Following the selection of our \"best\" parameters, we proceed to evaluate the model's performance using these optimized settings on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWVYuxHqpwmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44bb11a8-8d80-48a6-d324-3a885d34a06a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.433 |  Test Acc: 86.98%\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(model_save_path))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion, TAG_PAD_IDX)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
