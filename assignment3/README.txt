Ryo Novoa
Net ID anovoa
Student ID 31582858

Overview: Utilize the Bidirectional Encoder Representations from Transformers model (BERT) in the questional answering task. The project involves fine-tuning BERT for question answering and evaluating its performance on the Stanford Question Answering Dataset (SQuAD).

Tasks:
1 Fine-tuning BERT
Add a projection layer to the end of BERT which will map BERT's contextual embeddings into the text span of the answer. Projection layer must be implemented with different neural architectures, including a linear layer and a two-layer Multilayer Perceptron (MLP) with the ReLU activation function.

2 Question Answering and SQuAD Dataset
Present some data examples of SQuAD dataset in your report.

3 Evaluation Metrics
Design metrics to measure BERT's performance. These include a macro-averaged F1 score to measure average overlap between the prediction and ground truth answer, and exact match (EM) to measure the percentage of questions for which the model's answer is an exact match to the gold-standard answer. 

